# Zubov's Method Implementation with Physics-Informed Neural Networks

A JAX-based implementation of Zubov's method for determining the domain of attraction of dynamical systems using Physics-Informed Neural Networks (PINNs).

## Mathematical Formulation

### Zubov's Equation

The method solves the partial differential equation:

```math
\nabla W(\mathbf{x}) \cdot \mathbf{f}(\mathbf{x}) = -\omega(\mathbf{x}) \psi(W(\mathbf{x}))(1 - W(\mathbf{x}))
```

Where:
- $W(\mathbf{x})$: Zubov function (approximated by neural network)
- $\mathbf{f}(\mathbf{x})$: Dynamical system vector field
- $\omega(\mathbf{x}) = |\mathbf{x}|^2$: Positive definite function
- $\psi(W) = 0.1(1 + W)$: Scaling function

### Boundary Conditions

- $W(\mathbf{0}) = 0$: Origin condition
- $W(\mathbf{x}) < 1$: Domain of attraction
- $W(\mathbf{x}) = 1$: Boundary of domain

## Data Generation

### Trajectory Integration

Training data is generated by solving the dynamical system from random initial conditions:

```math
\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}), \quad \mathbf{x}(0) = \mathbf{x}_0
```

Using adaptive ODE solvers (DiffRax) with event detection for:

- **Equilibrium detection**: $\|\frac{d\mathbf{x}}{dt}\| < \epsilon$
- **Divergence detection**: $\|\mathbf{x}\| > M_{\text{max}}$
- **Convergence to origin**: $\|\mathbf{x}\| < \delta$

### Lyapunov Function Approximation

For trajectories converging to the origin, compute the integral:

```math
V(\mathbf{x}_0) = \int_0^{T_{\text{conv}}} \omega(\mathbf{x}(t)) dt = \int_0^{T_{\text{conv}}} \|\mathbf{x}(t)\|^2 dt
```

Where $T_{\text{conv}}$ is the time when the trajectory reaches the origin.

### Zubov Function Construction

The training data consists of pairs $(\mathbf{x}_0, W_{\text{target}}(\mathbf{x}_0))$ where:

```math
W_{\text{target}}(\mathbf{x}_0) = \begin{cases}
\beta(V(\mathbf{x}_0)) & \text{if trajectory converges} \\
1 & \text{if trajectory diverges or reaches equilibrium other than the origin}
\end{cases}
```

Common choices for $\beta$: $\tanh(0.1 \cdot V)$ or $1 - \exp(-0.1 \cdot V)$

## Test System: Van der Pol Oscillator

```math
\begin{align}
\dot{x}_1 &= -x_2 \\
\dot{x}_2 &= x_1 - \mu(1 - x_1^2)x_2
\end{align}
```

## Physics-Informed Loss Function

The neural network is trained using a composite loss function:

```math
\mathcal{L} = \mathcal{L}_{\text{PDE}} + \mathcal{L}_{\text{BC}} + \mathcal{L}_{\text{data}}
```

Where:

```math
\mathcal{L}_{\text{PDE}} = \mathbb{E}\left[(\nabla W \cdot \mathbf{f} + \psi(W)\omega(\mathbf{x})(1-W))^2\right]
```

```math
\mathcal{L}_{\text{BC}} = W(\mathbf{0})^2
```

```math
\mathcal{L}_{\text{data}} = \mathbb{E}\left[(W(\mathbf{x}_{\text{data}}) - y_{\text{data}})^2\right]
```

## Implementation

### Key Features

- **JAX optimization**: JIT compilation and vectorization
- **Efficient training**: `jax.fori_loop` for 2x-10x speedup
- **Event-driven integration**: Robust trajectory computation
- **GPU acceleration**: CUDA support through JAX

## Usage

```python
# 1. Generate training data
data_points = batched_solve(num_points=300, ...)

# 2. Create model
model = Net(din=2, layers_width=[10, 10, 10], dout=1, rngs=nnx.Rngs(1100))
optimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-3))

# 3. Train
model, optimizer, loss = train_model(
    model, optimizer,
    num_points=320_000, batch_size=32,
    data_points=data_points, system=system,
    omega=omega, psi=psi, domain=domain,
    args=args, key=jax.random.PRNGKey(0)
)

# 4. Visualize
plot_model_and_system(model, system, domain, args, 1000)
```

## References

Liu, J., Meng, Y., Fitzsimmons, M., & Zhou, R. (2025). *Physics-Informed Neural Network Lyapunov Functions: PDE Characterization, Learning, and Verification*. arXiv preprint arXiv:2312.09131. [https://arxiv.org/abs/2312.09131](https://arxiv.org/abs/2312.09131)